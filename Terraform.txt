when we use teraform 
lets say a company needs 50 servers u created in a short time developer created it using Aws Cloud Fromation Templates(Aws CAF) but after a period of time they think they need to move to the Azure then u need to transfer all the 50 servers to the Azure using azure resource manager(rm)
Then the developer Automated  all the servers form Aws to Azure in a normal manner but the problem is the now needs to move to on-premises we can do that using "Heat templates" this is one scenario   here is takes lots of time to do that 

Second Scernario is  we gone to hybrid cloud aws and then automated to azure and then to on-permises to overcome these and to compelete the task in a more efficent way it is using  "Terraform" 


Terraform is "Api As a Code"

hashicorp terraform coorporation 

sudo apt=get update,sude apt-get install -y gnupgsoftare-properties-common


lets say first u need to go throught the documentation of the terrform by haschicorp u need to remember all the things because there are more than 200 services in aws so when ever the interviwer asks this just say i am going through the documentation that is provided by haschicorp to perform all these kind of operations 

lets say create a vm in the ec2 and then i created ubuntu machine then i connected to it then install terraform into it by using  "sudo apt=get update,sude apt-get install -y gnupgsoftare-properties-common"
   and then create a folder and in that create two sub-folders whenever we are working on terraform we have these both states to update the mnain.tf terraform when we chage the 50 server form aws to azure or some other flatfrom then these folders comes into a place 

then in local create main.tf in that paste the code that is provided by the terraform documenatation and then do "terraform init"   this will intialize the terraform script and then "terraform apply"  then it will execute the code and then the acoording the written code in the code if u written to create ec2 then the ec2 will be created 

main.tf :--
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Configure the AWS Provider
provider "aws" {
  region = "us-east-1"
}

# Create a VPC
resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}



terraform init
  terraform plan 
     terraform apply
        terraform destroy

for additional information about the output.tf and variables.tf go through the 
   "https://github.com/futurice/terraform-examples/blob/master/aws/aws_lambda_api/main.tf"


now lets had a question like what is output.tf and variables.tf 
  output.tf in thg=ese we will write about what we are creating through the terraform script that is "main.tf"
  varibles.tf in these we will take all the variables and we will link it through the main.tf without writing dirctly into main.tf 


in these we go the context like in we need to store the state file in the backend or in vm not at the main machine for this we will use the remote-state folder and we will create a main.tf in that code is to create a s3 bucket and dynamo db 

the main.tf in remote-state folder:
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# CREATE AN S3 BUCKET AND DYNAMODB TABLE TO USE AS A TERRAFORM BACKEND
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# ----------------------------------------------------------------------------------------------------------------------
# REQUIRE A SPECIFIC TERRAFORM VERSION OR HIGHER
# This module has been updated with 0.12 syntax, which means it is no longer compatible with any versions below 0.12.
# This module is forked from https://github.com/gruntwork-io/intro-to-terraform/tree/master/s3-backend
# ----------------------------------------------------------------------------------------------------------------------

terraform {
  required_version = ">= 0.12"
}

# ------------------------------------------------------------------------------
# CONFIGURE OUR AWS CONNECTION
# ------------------------------------------------------------------------------

provider "aws" {}

# ------------------------------------------------------------------------------
# CREATE THE S3 BUCKET
# ------------------------------------------------------------------------------

data "aws_caller_identity" "current" {}

locals {
  account_id    = data.aws_caller_identity.current.account_id
}

resource "aws_s3_bucket" "terraform_state" {
  # With account id, this S3 bucket names can be *globally* unique.
  bucket = "${local.account_id}-terraform-states"

  # Enable versioning so we can see the full revision history of our
  # state files
  versioning {
    enabled = true
  }

  # Enable server-side encryption by default
  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"
      }
    }
  }
}

# ------------------------------------------------------------------------------
# CREATE THE DYNAMODB TABLE
# ------------------------------------------------------------------------------

resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-lock"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}


Discs of Terraform : 

- State file is single source of truth.
- Manual changes to the cloud provider cannot be identified and
auto-corrected.
- Not a GitOps friendly tool. Don't play well with Flux or Argo CD.
- Can become very complex and difficult to manage
- Trying to position as a configuration management tool as well.



